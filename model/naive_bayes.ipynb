{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理\n",
    "- 分词\n",
    "- 去停用词\n",
    "- 保存train.txt,test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import jieba\n",
    "\n",
    "DATA_PATH = '../sub-THUCNews'\n",
    "STOP_SIGNALS = {'\\n','\\u3000','\\xa0',' ',}\n",
    "def load_stop_words(stop_words_path='stopword.txt',stop_signals={'\\n','\\u3000','\\xa0',' ',}):\n",
    "    with open(stop_words_path,encoding='UTF-8-sig') as f:\n",
    "        s_stop_words = set()\n",
    "        for line in f:\n",
    "            s_stop_words.add(line.strip())\n",
    "        s_stop_words.update(stop_signals)\n",
    "        return s_stop_words\n",
    "    \n",
    "def tokenization(text):\n",
    "    '''\n",
    "    给定一段文本，返回分词结果\n",
    "    '''\n",
    "    return jieba.cut(text)\n",
    "\n",
    "def process(save_path, sub_path, s_stop_words,category, l_files):\n",
    "    res_f = open(save_path,'a',encoding='UTF-8-sig')\n",
    "    for file in l_files:\n",
    "            with open(sub_path+'/'+file,'r',encoding='utf-8') as f:\n",
    "                token_res = []          \n",
    "                for word in tokenization(f.read()):\n",
    "                    if word not in s_stop_words:\n",
    "                        token_res.append(word)\n",
    "                res_f.write(category+' '+ ' '.join(token_res)+'\\n')\n",
    "                \n",
    "    res_f.close()\n",
    "    \n",
    "def text_preprocessing(data_path='../sub-THUCNews'):\n",
    "    '''\n",
    "    读取文件数据，进行分词，去停用，保存\n",
    "    '''\n",
    "    s_stop_words = load_stop_words()\n",
    "    l_categories = os.listdir(data_path)\n",
    "    for category in l_categories:\n",
    "        sub_path = data_path + '/'+category\n",
    "        l_files = random.shuffle(os.listdir(sub_path))  \n",
    "        process('train.txt',sub_path, s_stop_words,category, l_files[:int(0.8*len(l_files))])  # 生成训练集数据\n",
    "        process('test.txt',sub_path, s_stop_words,category, l_files[int(0.8*len(l_files)):])  # 生成测试集数据\n",
    "    \n",
    "text_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计信息\n",
    "- 统计每个类别文档数量\n",
    "- 统计单词-类别文档数量\n",
    "- 统计每个文档中词频信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'体育': 19}\n",
      "120\n",
      "[Counter({'女足': 14, '韦迪': 5, '男足': 5, '请': 3, '外籍': 3, '商瑞华': 3, '永川': 3, '体育局': 3, '会议': 3, '出线': 2, '下课': 2, '召开': 2, '研讨': 2, '12': 2, '省市': 2, '中国女足': 2, '困难': 2, '本报': 1, '专电': 1, '记者': 1, '孙永军': 1, '明天': 1, '会同': 1, '洪臣': 1, '薛立': 1, '两位': 1, '主管': 1, '副': 1, '主任': 1, '前往': 1, '大会': 1, '参加': 1, '此次': 1, '加快': 1, '步伐': 1, '观摩': 1, '24': 1, '日': 1, '热身赛': 1, '之机': 1, '足管': 1, '领导班子': 1, '齐聚': 1, '高': 1, '与会者': 1, '国脚': 1, '足协': 1, '负责人': 1, '本次': 1, '两个': 1, '议题': 1, '一是': 1, '举国体制': 1, '难题': 1, '全运会': 1, '金牌': 1, '刺激': 1, '鼓励': 1, '二是': 1, '现实': 1, '球队': 1, '内外部': 1, '步调': 1, '昨天': 1, '表示': 1, '后备力量': 1, '薄弱': 1, '关注度': 1, '低': 1, '匮乏': 1, '国家队': 1, '队员': 1, '透露': 1, '物色': 1, '想着': 1, '难度': 1, '由国': 1, '管部': 1, '按部就班': 1, '想': 1, '关键': 1, '距离': 1, '亚洲杯': 1, '开战': 1, '一个月': 1, '解压': 1, '大战': 1, '前': 1, '折损': 1, '大将': 1, '马晓旭': 1, '比赛': 1, '很大': 1, '中国足协': 1, '肯定': 1})]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "l_categories = ['财经','彩票','房产','股票','家居','教育','科技','社会','时尚','时政','体育','星座','游戏','娱乐']\n",
    "\n",
    "def info_statis():\n",
    "    d_words_count = {}\n",
    "    d_doc_count = {}\n",
    "    l_words_counters = []\n",
    "    with open('train.txt',encoding='UTF-8-sig') as f:\n",
    "        for line in f:\n",
    "            label, content = line.strip().split(' ',1)\n",
    "            words_counter = collections.Counter(content.split(' '))\n",
    "            l_words_counters.append(words_counter)\n",
    "            if label in d_doc_count:\n",
    "                d_doc_count[label] += 1\n",
    "            else:\n",
    "                d_doc_count[label] = 1\n",
    "    \n",
    "            for word in words_counter.keys():\n",
    "                if word in d_words_count and label in d_words_count[word].keys():\n",
    "                    # 已存在相应单词\n",
    "                    d_words_count[word][label] += 1  # 属于label类且对应item的文档数量\n",
    "\n",
    "                elif word in d_words_count and label not in d_words_count[word].keys():\n",
    "                    d_words_count[word][label] = 1\n",
    "                \n",
    "                else:\n",
    "                    d_words_count[word] = {}\n",
    "                    d_words_count[word][label] = 1\n",
    "    \n",
    "    return {\n",
    "        'd_words_count':d_words_count,\n",
    "        'd_doc_count':d_doc_count,\n",
    "        'l_words_counters':l_words_counters\n",
    "    }\n",
    "\n",
    "res = info_statis()\n",
    "print(res['d_words_count']['女足'])\n",
    "print(res['d_doc_count']['体育'])\n",
    "print(res['l_words_counters'][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取\n",
    "- 特征选择  \n",
    "    依据互信息选择每个类别前500个特征词作为特征项\n",
    "- 特征权重计算    \n",
    "    依据TF-IDF计算每个特征的权重  \n",
    "- 特征和权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['比赛', '前', '球队', '球员', '之后', '国家队', '体育讯', '新', '队', '联赛', '男篮', '对手', '足球', '俱乐部', '最后', '足协', '队员', '之前', '曾', '主教练', '来说', '一场', '中国足协', '教练', '赛季', '球迷', '训练', '亚运会', '热身赛', '肯定', '一次', '原因', '主帅', '无法', '中国男篮', '决定', '主任', '只能', '备战', '刚刚', '世界杯', '冠军', '韦迪', '曾经', 'CBA', '女足', '体育', '重新', '提出', '阶段', '讯', '新浪', '12', '观众', '音乐', '电视剧', '歌曲', '这次', '导演', '拍摄', '演唱会', '歌手', '透露', '故事', '2010', '电影', '文', '角色', '举行', '节目', '专辑', '歌迷', '此次', '唱', '卫视', '表演', '成', '该剧', '经典', '演绎', '人物', '演唱', '采访', '笑', '一部', '剧中', '情感', '听', '饰演', '戏', '发行', '邀请', '剧', '播出', '这部', '讲述', '首次', '主持人', '组合', '亮相', '地板', '家居', '木地板', '装饰', '环保', '实', '舒适', '核心', '装修', '厨房', '实木', '材料', '红', '开启', '家装', '电器', '智能', 'www', '营销', '绿色', '温馨', '宅', '德意', '诞生', '安装', '线条', '客厅', '木材', '装', '酒吧', '式', '餐厅', '复古', '奢华', '简约', '混搭小窝', '铺装', '橡', '森林', '一块', '清洁', '大气', '盛大', '背景墙', '专卖店', '188G', '久盛', '美式', '原味', '质感', '10', '期', '14', '奖金', '本期', '注', '奖', '彩票', '双色球', '20', '开奖', '30', '号码', '号', '开出', '头奖', '中奖', '投注', '主场', '13', '一等奖', '场', '17', '16', '微博', '大奖', '22', '球', '客场', '19', '分别', '31', '不败', '二等奖', '足彩', '18', '09', '区', '负', '24', '看好', 'VS', '两队', '胜', '兑奖', '销量', '当期', '中出', '全部', '06', '户型', '样板间', '点评', '论坛', '相册', '平米', '置业', '开发商', '房产', '最终', '公布', '直接', '房', '为准', '动向', '邮箱', '更多', '订阅', '地图搜索', '热盘', '仅供参考', '刊', '发送到', '编辑', '居', '均价', '元', '精装', '万', '盘', '通透', '乐居', '淘', '板楼', '稿件', '转载', '引用', '版权所有', '注明', '出处', '2011', '火热', '线', '入住', '50', '预计', '90', '京城', '70', '产权', '考试', '年', '考生', '情况', '调整', '变化', '请', '说明', '公务员', '正式', '访问', '圈', '博客', '权威部门', '新浪网', '频道', '敬请', '教育', '自考', '参加', '成绩', '报考', '2008', '学生', '规定', '职业', '分', '报名', '自学', '学习', '结束', '08', '课程', '通知', '查询', '笔试', '人数', '毕业', '考', '安排', '老师', '科目', '机关', '同学', '面试', '大学', '市', '毕业生', '职位', '理解', '组图', '导语', '吃', '身体', '减肥', '女性', '食物', '使', '运动', '每天', '体重', '身材', '脂肪', '性感', '瘦身', '食品', '热量', '喝', '水果', '营养', '含有', '外', '饮食', '引起', '体内', '女星', '肌肉', '皮肤', '少', '食用', '维生素', '瘦', '动作', '含', '含量', '蔬菜', '补充', '能量', '消耗', '功效', '警惕', '蛋白质', '美丽', '感染', '疾病', '防止', '物质', '冬季', '人体', '食谱', '日', '月', '报道', '发生', '观看', '首页', '图酷', '精彩图片', '日电', '21', '死亡', '新华社', '造成', '2009', '名', '当天', '中新网', '地区', '总统', '首都', '在线', '袭击', '新华网', '遭', '每日', '专稿', '北部', '发言人', '当日', '民众', '外电报道', '驻', '美军', '军队', '印度', '东部', '现年', '法新社', '军方', '邮报', '类似', '领导人', '选举', '发', '反对', '该国', '士兵', '总理', '短暂', '地震', '图', '人', '中', '好', '最', '做', '想', '容易', '觉得', '心理', '喜欢', '却', '对方', '事情', '爱', '爱情', '一种', '心情', '再', '感情', '这种', '事', '知道', '星座', '感觉', '情绪', '表现', '一起', '男人', '身边', '机会', '总是', '比较', '女人', '应该', '感到', '产生', '走', '注意', '人们', '一下', '愿意', '这是', '之间', '太', '变得', '相处', '恋爱', '双子座', '狮子座', '游戏', '玩家', '玩', '近日', '官方', '网络', 'PS3', '网络游戏', '视频', '发售', '网游', '登陆', '上市', 'PC', '运营', '将会', '日前', '玩游戏', '加入', '版', '厂商', '社交', '主机', '同样', '预定', '版本', '索尼', '动漫', '达', '官方网站', '任天堂', '让玩家', '巨大', '正是', '逐渐', 'http', '3DS', 'Xbox', 'Xbox360', '装备', '确认', '3D', '软件', 'net', 'QQ', 'Wii', '下载', '日元', 'ChinaJoy', 'chinajoy', '记者', '后', '说', '时', '一名', '岁', '发现', '已', '称', '里', '看到', '今年', '男子', '当时', '警方', '告诉', '昨日', '接到', '随后', '钱', '来到', '找', '本报讯', '左右', '孩子', '两个', '离开', '找到', '下午', '附近', '突然', '儿子', '医院', '只', '准备', '上午', '电话', '回家', '家人', '家里', '昨天', '晚上', '有人', '跑', '再次', '父亲', '两人', '回来', '负责人', '工作人员', '屏幕', '买', '重量', '位', '评论', '点击', '寸', '观望', 'COUNT', '一款', '公斤', '大小', '笔记本', '处理器', '款', '硬盘', '图片', '文章', '平台', '参数', '热评', '内存', '英寸', '双核', '【', '】', '诺基亚', '运营商', '中端', '移动', '智能手机', '操作系统', '显卡', '行情', '微软', '苹果', '无线', '酷睿', '机型', 'Android', 'Windows', 'Intel', '通信', '摄像头', '英特尔', '经销商', '光驱', '机身', '这款', '像素', '消息', '财经', '表示', '亿美元', '11', '去年', '显示', '银行', '下降', '增长', '数据', '下滑', '此前', '月份', '美', '宣布', '这一', '报告', '晚间', '平均', '分析师', '经济衰退', '万美元', '贷款', '减少', '美国政府', '唐风', '过去', '指出', '凌晨', '首席', '专题', '经济学家', '刺激', '措施', '奥巴马', '创下', '失业率', '周二', '美联储', '一项', '明年', '预测', '数字', '低于', '复苏', '衰退', '纽约', '抵押', '削减', '影响', '下', '投资', '出现', '高', '上涨', '风险', '认为', '期货', '预期', '压力', '受', '下跌', '明显', '近期', '大幅', '基金', '因素', '美元', '存在', '导致', '面临', '上升', '连续', '点', '走势', '100', '股市', '股票', '相对', '存款', '合约', '建议', '金融', '支撑', '涨幅', '油价', '人民币', '指数', '短期', '收益', '理财产品', '约', '回落', '利率', '央行', '形势', '金融危机', '一年', '至少']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "\n",
    "def get_MI(word, category, d_words_count, d_doc_count):\n",
    "    N = sum([v for v in d_doc_count.values()])\n",
    "    try:\n",
    "        A = d_words_count[word][category]\n",
    "    except KeyError:\n",
    "        A = 0 \n",
    "    C = d_doc_count[category] - A\n",
    "    B = N - d_doc_count[category]- A\n",
    "    D = N - C - B -A\n",
    "    return A*N/((A+C)*(A+B))\n",
    "\n",
    "def feature_selection(d_words_count,d_doc_count):\n",
    "    '''\n",
    "    依据互信息最大的选择500个特征,返回特征语料库\n",
    "    '''\n",
    "#     d_words_count = {'财经':{},'彩票':{},'房产':{},'股票':{},'家居':{},'教育':{},'科技':{},'社会':{},'时尚':{},'时政':{}\n",
    "#                      ,'体育':{},'星座':{},'游戏':{},'娱乐':{}}  # {'财经': {'油价': 23, '高企将': 1,...\n",
    "    \n",
    "    l_corpus = []    \n",
    "    # 计算互信息\n",
    "    catedory_word_MIs = {'财经':[],'彩票':[],'房产':[],'股票':[],'家居':[],'教育':[],'科技':[],'社会':[],'时尚':[],'时政':[],\n",
    "                      '体育':[],'星座':[],'游戏':[],'娱乐':[]}\n",
    "    \n",
    "    \n",
    "    for word in d_words_count.keys():\n",
    "        max_MI, closest_category = 0, None\n",
    "        for category in d_doc_count.keys():\n",
    "            MI = get_MI(word,category,d_words_count, d_doc_count)\n",
    "            if MI > max_MI:\n",
    "                max_MI = MI \n",
    "                closest_category = category\n",
    "        catedory_word_MIs[closest_category].append((word,max_MI)) \n",
    "    \n",
    "    # 对各个类别选取前500个作为特征词\n",
    "    for category in d_doc_count.keys():\n",
    "        l_corpus.extend([x[0] for x in sorted(catedory_word_MIs[category], key=lambda x:x[1], reverse=True)[:50]])\n",
    "    \n",
    "    return l_corpus\n",
    "    \n",
    "def weight_calculation(l_corpus,l_words_counters,d_words_count,d_doc_count):\n",
    "    '''\n",
    "    依据TF-IDF值计算每个特征在分类中的权重，在贝叶斯中没用到\n",
    "    '''\n",
    "    all_counter = collections.Counter()\n",
    "    for counter in l_words_counters:\n",
    "        all_counter += counter\n",
    "    \n",
    "    N = sum([v for v in d_doc_count.values()])  # 总的文档数\n",
    "    word_tf_idf = {}\n",
    "    for word in l_corpus:\n",
    "        idf = math.log(N/sum([v for v in d_words_count[word].values()]))\n",
    "        word_tf_idf[word] = all_counter.get(word)*idf\n",
    "    \n",
    "    return word_tf_idf\n",
    "    \n",
    "    \n",
    "def feature_extraction():\n",
    "    '''\n",
    "    遍历train.txt，统计每个词在每个类别中出现次数，利用互信息选择特征，TF-IDF值计算特征权重\n",
    "    '''\n",
    "    info = info_statis()\n",
    "    l_corpus = feature_selection(info['d_words_count'], info['d_doc_count'])\n",
    "#     d_features = weight_calculation(l_corpus,info['l_words_counters'],info['d_words_count'],info['d_doc_count']) # 每个特征及其对应的权重\n",
    "#     return d_features\n",
    "    return l_corpus\n",
    "\n",
    "def save_features(features):\n",
    "    with open('features.json','w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(features))\n",
    "features = feature_extraction()\n",
    "save_features(features)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练分类器\n",
    "- 贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'体育': 120, '娱乐': 120, '家居': 120, '彩票': 120, '房产': 120, '教育': 120, '时尚': 120, '时政': 120, '星座': 120, '游戏': 120, '社会': 120, '科技': 120, '股票': 120, '财经': 120}\n"
     ]
    }
   ],
   "source": [
    "C = 14  # 设定的类的数量\n",
    "\n",
    "import collections\n",
    "\n",
    "def load_features():\n",
    "    with open('features.json',encoding='utf-8') as f:\n",
    "        features = json.loads(f.read())\n",
    "    return features\n",
    "\n",
    "def fit_transform(features, words_counter):\n",
    "    '''\n",
    "    将输入的文档依据特征集合转换为向量形式\n",
    "    '''\n",
    "    doc_feature_vector = []\n",
    "    for feature in features:\n",
    "        if words_counter.get(feature):\n",
    "            doc_feature_vector.append(words_counter.get(feature))\n",
    "        else:\n",
    "            doc_feature_vector.append(0)\n",
    "    return doc_feature_vector\n",
    "\n",
    "def transform(features, file_path):\n",
    "    '''\n",
    "    接受特征字典{'feature':weight}和文件（训练或测试），将数据转换为向量表示返回\n",
    "    '''\n",
    "    d_categories = {}  # 统计每个类的数量信息\n",
    "    l_category_vectors = []  # 统计每个类和特征向量信息\n",
    "    with open(file_path, encoding='UTF-8-sig') as f:\n",
    "        for line in f:\n",
    "            label, content = line.strip().split(' ',1)\n",
    "            words_counter = collections.Counter(content.split(' '))\n",
    "            doc_feature_vector = fit_transform(d_features, words_counter)\n",
    "            d_category_vector = {\n",
    "                'category':label,\n",
    "                'vector':doc_feature_vector\n",
    "            }\n",
    "            l_category_vectors.append(d_category_vector)\n",
    "            try:\n",
    "                d_categories[label] += 1\n",
    "            except KeyError:\n",
    "                d_categories[label] = 1\n",
    "            \n",
    "        return {\n",
    "            'd_categories':d_categories,  # 每个类别的统计信息，用于计算先验概率\n",
    "            'l_category_vectors':l_category_vectors  # 由类别和特征向量组成的字典列表\n",
    "        }\n",
    "           \n",
    "features = load_features()\n",
    "trans_res = transform(features,'train.txt')\n",
    "print(trans_res['d_categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training..\n",
      "testing...\n",
      "测试数据：420条\n",
      "准确率：0.8714285714285714\n",
      "星座\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def load_stop_words(stop_words_path='stopword.txt',stop_signals={'\\n','\\u3000','\\xa0',' ',}):\n",
    "    with open(stop_words_path,encoding='UTF-8-sig') as f:\n",
    "        s_stop_words = set()\n",
    "        for line in f:\n",
    "            s_stop_words.add(line.strip())\n",
    "        s_stop_words.update(stop_signals)\n",
    "        return s_stop_words\n",
    "    \n",
    "def get_prior_prb(d_categories):\n",
    "    '''\n",
    "    计算先验概率，接受由所有分类以及对应数量组成的字典\n",
    "    '''\n",
    "    d_prior_prb = {}  # P(cj)\n",
    "    N_all = sum(d_categories.values())\n",
    "    for category,N_c in d_categories.items():\n",
    "        d_prior_prb[category] = (1+N_c)/(C+N_all)\n",
    "    \n",
    "    return d_prior_prb\n",
    "\n",
    "def get_likelihood_prb(l_category_vectors):\n",
    "    '''\n",
    "    计算似然概率，接受由category和特征向量组成的列表\n",
    "    '''\n",
    "    d_likelihood_prb = {}   # P(wi|cj)\n",
    "    d_feature_category = {}  # 统计某一特征对应分类的分量\n",
    "    d_all_category = {}  # 统计所有特征对应分类的分量\n",
    "    for d_category_vector in l_category_vectors:\n",
    "        category = d_category_vector['category']\n",
    "        vector = d_category_vector['vector']\n",
    "        for i,v in enumerate(vector):\n",
    "            try:\n",
    "                d_feature_category[(i,category)] += v\n",
    "            except KeyError:\n",
    "                d_feature_category[(i,category)] = v\n",
    "\n",
    "            finally:\n",
    "                if category not in d_all_category:\n",
    "                    d_all_category[category] = v\n",
    "                else:\n",
    "                    d_all_category[category] += v\n",
    "\n",
    "    M = len(vector)\n",
    "\n",
    "    for k,v in d_feature_category.items():\n",
    "        d_likelihood_prb[k] = (1+v)/(M + d_all_category[k[1]]) \n",
    "    return d_likelihood_prb\n",
    "\n",
    "def get_predict_prb(prior_prb,likelihood_prb, categories, l_category_vectors):\n",
    "    '''\n",
    "    基于先验概率和条件概率计算\n",
    "    '''\n",
    "    count = 0 \n",
    "    total = len(l_category_vectors)\n",
    "    print('测试数据：{}条'.format(total))\n",
    "    for d_category_vector in l_category_vectors:\n",
    "        # 对每一条特征做分类\n",
    "        true_category = d_category_vector['category']\n",
    "        vector = d_category_vector['vector']\n",
    "        i =0\n",
    "        max_predict_prb = 0\n",
    "        predict_category = None\n",
    "        for category in categories:\n",
    "            # 计算该文本在各个分类下的概率\n",
    "            prod_likelihood_prb = 1\n",
    "            for i,v in enumerate(vector):\n",
    "                # 对向量中每一个不为0的分量求概率\n",
    "                if v != 0 :\n",
    "                    prod_likelihood_prb *= likelihood_prb[(i,category)]\n",
    "            predict_prb = prior_prb[category]*prod_likelihood_prb\n",
    "#             print('category:{},prb:{}'.format(category, predict_prb))\n",
    "            if predict_prb > max_predict_prb:\n",
    "                predict_category = category  # 更新概率最大类别\n",
    "                max_predict_prb = predict_prb\n",
    "#         print('true_category:{},predict_category:{}'.format(true_category,predict_category))\n",
    "        if predict_category == true_category:\n",
    "            count += 1\n",
    "    print('准确率：{}'.format(count/total))\n",
    "    \n",
    "class NaiveBayes: \n",
    "    \n",
    "    def __init__(self):\n",
    "        self._features = self.__load_features()\n",
    "        self._prior_prb = {}  # 各个分类的先验概率\n",
    "        self._likelihood_prb = {}  # 各个特征在不同分类下的概率\n",
    "        self._l_categories = ['财经','彩票','房产','股票','家居','教育','科技','社会','时尚','时政','体育','星座','游戏','娱乐']   \n",
    "        \n",
    "        self._train_path = 'train.txt'\n",
    "        self._test_path = 'test.txt'\n",
    "        \n",
    "        self._s_stop_words = None\n",
    "    \n",
    "    def __load_features(self):\n",
    "        with open('features.json',encoding='utf-8') as f:\n",
    "            features = json.loads(f.read())\n",
    "        return features\n",
    "    \n",
    "    def _tokenization(self,text):\n",
    "        return jieba.cut(text)\n",
    "    \n",
    "    def _fit_transform(self,words_counter):\n",
    "        '''\n",
    "        将输入的文档依据特征集合转换为向量形式\n",
    "        '''\n",
    "        doc_feature_vector = []\n",
    "        for feature in self._features:\n",
    "            if words_counter.get(feature):\n",
    "                doc_feature_vector.append(words_counter.get(feature))\n",
    "            else:\n",
    "                doc_feature_vector.append(0)\n",
    "        return doc_feature_vector\n",
    "    \n",
    "    def _text_preprocess(self,text):\n",
    "        '''\n",
    "        预处理数据，返回文本的特征向量表达形式\n",
    "        '''\n",
    "        if not self._s_stop_words:\n",
    "            self._s_stop_words = load_stop_words()\n",
    "        token_res = []          \n",
    "        for word in self._tokenization(text): # 分词\n",
    "            if word not in self._s_stop_words:  # 去停用词\n",
    "                token_res.append(word)\n",
    "        words_counter = Counter(token_res)\n",
    "        return self._fit_transform(words_counter)\n",
    "        \n",
    "    def train(self):\n",
    "        print('training..')\n",
    "        transform_res = transform(self._features,self._train_path)\n",
    "        self._prior_prb = get_prior_prb(transform_res['d_categories'])\n",
    "        self._likelihood_prb = get_likelihood_prb(transform_res['l_category_vectors'])\n",
    "        \n",
    "    def test(self):\n",
    "        print('testing...')\n",
    "        transform_res = transform(self._features, self._test_path)\n",
    "        get_predict_prb(self._prior_prb, self._likelihood_prb, self._l_categories, transform_res['l_category_vectors'])\n",
    "        \n",
    "    def predict(self,text):\n",
    "        vector = self._text_preprocess(text)\n",
    "        max_predict_prb = 0\n",
    "        for category in self._l_categories:\n",
    "            # 计算该文本在各个分类下的概率\n",
    "            prod_likelihood_prb = 1\n",
    "            for i,v in enumerate(vector):\n",
    "                # 对向量中每一个不为0的分量求概率\n",
    "                if v != 0 :\n",
    "                    prod_likelihood_prb *= self._likelihood_prb[(i,category)]\n",
    "            predict_prb = self._prior_prb[category]*prod_likelihood_prb\n",
    "#             print('category:{},prb:{}'.format(category, predict_prb))\n",
    "            if predict_prb > max_predict_prb:\n",
    "                predict_category = category  # 更新概率最大类别\n",
    "                max_predict_prb = predict_prb\n",
    "        print(predict_category)\n",
    "        \n",
    "        \n",
    "naive_bayes = NaiveBayes()\n",
    "naive_bayes.train()\n",
    "naive_bayes.test()\n",
    "\n",
    "TEST_TEXT =\"\"\"\n",
    "婚恋心理：婆媳聊天必知的潜规则(图)\n",
    "　　婆媳矛盾是我们中华民族的千古矛盾，一直都得不到缓解。这个社会的人都有两面性，大家嘴上说着一套一套的漂亮话，但是实际上所作所为，又是另外一回事。而婆媳关系也有两套规则，一套是明规则，还有一套潜规则，利用好了，这个千古矛盾对你来说将不再是难题。\n",
    "　　婆媳相处如何妙用“潜规则”\n",
    "　　可是，我们当中有多少人是口含银匙而生呢？多少人是公主下嫁招驸马的童话呢我们当中的大多数，不都是要为柴米油盐生计而喜怒哀乐吗？不都要正视如何和婆家人相处——我们不想可又不得不去做吗？\n",
    "　　首先，我建议婆媳之间不要直接交流，有什么相左的意见应该通过先生缓冲一下，他是“汉奸”——会和皇军交流，也懂八路的心思。\n",
    "　　其次，如果直接交流受阻，一定要先自省：自己冲不冲动，有没有言语不当的地方，对婆婆有没有肢体冲撞，自己如果和妈妈这么说，妈妈怎么反应如果这些都自省过了，没有问题，那就要和先生说，实事求是，注意方式，不要动怒说粗，宁可哭，不可以骂人。如果是自己做的过分，有形式上的不当之处，但是内容没有错，就得避重就轻一点了，但是要提。记住：提前招认，绝对好过后来被盘问不得不招。如果你有重大错误，对不起，我也不知道怎么办了。因为我从来不和婆婆正面交流不同意见。请其她姐妹指点吧。\n",
    "　　总之，要做和先生解释说明冲突的第一人，要尽量心平气和，决不能搞人身攻击，婆婆丰满说人家是吹了气的青蛙，公公苗条说人家是榨干的甘蔗。要会做人，尤其是有外人在的场合，要表现的温和有礼，听话勤快，既让婆婆有面子，也可以请外人给你制造舆论。\n",
    "　　婆媳相处，要善于利用“潜规则”\n",
    "　　婆媳交流，要注意不能乱用潜规则，尽量说漂亮的官话。哪怕虚伪点，也不能来个赤裸裸的大实话，起码，不能首先使用大实话。聪明的妈妈会教女儿嘴巴要甜，说白了就是要会说官话。\n",
    "　　当然，官话不仅仅是说话，还包括行动。例如一个五十多岁的媳妇得到了众人的赞扬，说她有媳妇相，自己都是有媳妇的人了，还那么孝顺婆婆。那她是怎么做的呢有客人来了，她贴身伺候婆婆，给婆婆拿着热水袋，香烟火柴，站在婆婆身边伺候着。其实，她这是在监视婆婆，让她没法说坏话，要说，只能说好话——这样，她的好名声就得到最权威的认可了。\n",
    "　　如果娘家和婆家势力悬殊，或是先生靠着爸爸提携，你就不用担心什么婆媳关系了，婆婆哪还敢说你坏话她得为儿子好啊。这种情况下，媳妇若是为长久计，就要锦上添花，待公婆好一些，省得老公翅膀硬了老爹退休了，公婆甚至老公一口恶气吐到脸上来。如果不想费力气，那也不用做什么，大家场面上过的去就行了。\n",
    "\"\"\"\n",
    "naive_bayes.predict(TEST_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
